# Lexile Corpus Tuner

Lexile Corpus Tuner now ships two complementary toolsets:

1. **Lexile Tuner CLI (`lexile-tuner`)** – a Lexile-inspired analysis and rewriting pipeline. It slices overlapping windows, scores them with a pluggable estimator, and optionally rewrites high-Lexile spans via an LLM-backed rewriter so the corpus fits a target band.
2. **Lexile-Faithful Text Difficulty Pipeline (`text-difficulty-pipeline`) – _in process_** – a full corpus/analyzer/calibration stack that mimics the official Lexile analyzer: it builds a proxy corpus (Gutenberg + Simple Wiki + curated OERs), computes word frequencies per 5 M tokens, extracts Lexile-style features (MSL/MLF), and fits a ridge regression against curated texts with known Lexile measures.

Both layers share the same normalization/tokenization code so that corpus frequencies and analyzer features stay aligned.

---

## Features

### Completed

- Tokenization with character offsets (round-trip safe for rewriting).
- Sliding-window segmentation and deterministic scoring pipeline.
- Pluggable estimator interface (dummy heuristic included, plus TensorFlow `lexile_v2`).
- Constraint detection for max window Lexile and document-level averages.
- Rewriter abstraction for OpenAI’s Responses API or a no-op stub.
- Typer-based CLI (`lexile-tuner`) with `analyze`, `rewrite`, and `print-config` commands.
- Corpus pipeline (`text-difficulty-pipeline corpus`):
  - Gutenberg downloader configurable via `data/meta/gutenberg_ids.txt` (strict-English list generated by `scripts/build_gutenberg_id_list.py`).
  - Simple English Wikipedia dump downloader + extractor (`scripts/extract_simple_wiki_dump.py`).
  - OpenStax / CK-12 manifest downloader (`data/meta/oer_sources.json`).
  - Shared normalization/sharding → `data/corpus/normalized/shards/`).
  - Word-frequency computation with per-source weighting (`data/meta/corpus_sources.json`).
- Analyzer + calibration stack:
  - Sentence segmentation, Lexile-style slice builder, MSL/MLF features, regression inference, special-case adjustments.
  - Calibration CLI commands to fetch curated texts, build datasets, train ElasticNet models, and export JSON specs.

### In Process

- Documentation parity between this README and `.github/text-difficulty-pipeline-plan.md`.
- Expanding open-license source coverage (additional CC-BY news/blog feeds, government texts).
- Enhanced calibration diagnostics (per-band MAE/RMSE, residual plots, automated sanity checks).
- Publishing turnkey examples for building/staging calibration catalogs (`data/calibration/catalog/lexile_catalog.csv`).

---

## Installation

```bash
poetry install --with dev
# Include TensorFlow adapter extras if you plan to use the lexile_v2 estimator
# poetry install --with dev --extras "lexile-v2"
# Include OpenAI extras if you plan to run the LLM-backed rewriter
# poetry install --with dev --extras "llm-openai"
```

Formatting uses `black` + `isort`; run `black .` and `isort .` (or rely on pre-commit hooks) before committing.

---

## Usage

### Lexile Tuner CLI

```bash
# Analyze a folder of .txt or .epub files
poetry run lexile-tuner analyze --input-path examples/example_corpus --config examples/example_config.yaml

# Rewrite difficult passages, producing tuned output
poetry run lexile-tuner rewrite --input-path examples/example_corpus --output-path artifacts/tuned --config examples/example_config.yaml

# Print the default configuration
poetry run lexile-tuner print-config

# Analyze a single EPUB
poetry run lexile-tuner analyze --input-path examples/example_corpus/pg2701-images-3.epub
```

### Text Difficulty Pipeline (Corpus + Analyzer)

```bash
# 0) (Optional) Generate a Gutenberg ID list from Gutendex (strict English)
poetry run python scripts/build_gutenberg_id_list.py

# 1) (Optional) Copy examples/meta/oer_sources.example.json → data/meta/oer_sources.json and fill in plain-text OpenStax/CK-12 URLs

# 2) Download raw sources (Gutenberg, Simple Wiki dump, OER manifest)
poetry run text-difficulty-pipeline corpus download

# 3) Convert Simple Wiki dump into JSONL articles
poetry run python scripts/extract_simple_wiki_dump.py \
  --dump data/corpus/raw/simple_wiki/simplewiki-latest-pages-articles.xml.bz2 \
  --output data/corpus/raw/simple_wiki/simplewiki_articles.jsonl

# 4) Normalize & shard
poetry run text-difficulty-pipeline corpus normalize --shard-size-tokens 100000

# 5) Compute word frequency table (honors source weights in data/meta/corpus_sources.json)
poetry run text-difficulty-pipeline corpus frequencies

# 6) Calibration workflow (once lexile_catalog.csv + texts are staged)
poetry run text-difficulty-pipeline calibration fetch-texts \
  --catalog data/calibration/catalog/lexile_catalog.csv \
  --texts-root data/calibration/texts

poetry run text-difficulty-pipeline calibration build-dataset \
  --catalog data/calibration/catalog/lexile_catalog.csv \
  --texts-root data/calibration/texts \
  --output data/calibration/calibration_dataset.parquet

poetry run text-difficulty-pipeline calibration fit \
  data/calibration/calibration_dataset.parquet \
  --out data/model/lexile_regression_model.json

# 7) Analyze new material via the Lexile-faithful analyzer
poetry run text-difficulty-pipeline analyze text path/to/doc.txt --json-output report.json
```

---

## Architecture Overview

### Lexile Tuner

1. **Tokenization** (`tokenization.py`) – regex tokenizer that emits offsets for rewriting.
2. **Windowing** (`windowing.py`) – overlapping windows derived from tokens per config.
3. **Scoring** (`scoring.py`) – applies a `LexileEstimator` to every window and computes document statistics.
4. **Constraints** (`constraints.py`) – detects max-window and document-average violations.
5. **Rewriting** (`rewriting.py`) – pluggable rewriter interface (OpenAI implementation available).
6. **Pipeline** (`pipeline.py`) – orchestrates tokenize → window → score → rewrite loop.
7. **CLI** (`cli.py`) – Typer wrapper exposing `analyze`, `rewrite`, `print-config`.

Custom estimators plug in via `lexile_corpus_tuner.estimators.create_estimator`.

### Text Difficulty Pipeline

1. **Corpus Builder**
   - `scripts/build_gutenberg_id_list.py`: Gutendex → `data/meta/gutenberg_ids.txt` generator (strict English by default).
   - `src/lexile_corpus_tuner/corpus/download.py`: downloads Gutenberg, Simple Wiki dumps, and OER manifest entries.
   - `scripts/extract_simple_wiki_dump.py`: streams the XML dump into JSONL articles (namespace 0, redirect-free).
   - `src/lexile_corpus_tuner/corpus/normalize.py`: shared normalization/tokenization + sharding.
   - `src/lexile_corpus_tuner/corpus/frequencies.py`: counts tokens, applies per-source weights, outputs TSV + metadata.
2. **Analyzer Layer** – slice construction (`analyzer/slices.py`), feature computation (`analyzer/features.py`), regression inference (`analyzer/model.py`), CLI interface (`analyzer/cli.py`).
3. **Calibration Layer** – engineered features, ElasticNet training, JSON model store, CLI for `fetch-texts`, `build-dataset`, `fit`. Calibration assets live under `data/calibration/`.

---

## Configuration

`lexile_corpus_tuner.config` exposes helpers to load `LexileTunerConfig` from dictionaries or YAML files. See `examples/example_config.yaml` for window/stride/estimator samples.

---

## Using the `lexile_v2` Estimator

To run the TensorFlow-based estimator you need artifacts exported from the upstream `lexile-determination-v2` training project. Install the extra dependencies (`poetry install --with dev --extras "lexile-v2"`), set `estimator_name: lexile_v2`, and supply the artifact paths in your config. CLI overrides such as `--lexile-v2-model-path` are supported.

```yaml
estimator_name: lexile_v2
lexile_v2_model_path: examples/lexile_v2_artifacts/model.h5
lexile_v2_vectorizer_path: examples/lexile_v2_artifacts/tokenizer.pickle
lexile_v2_label_encoder_path: examples/lexile_v2_artifacts/labels.pickle
lexile_v2_stopwords_path: examples/lexile_v2_artifacts/stopwords.pickle
lexile_v2_band_to_midpoint:
  200-299: 250
  300-399: 350
```

Artifacts are bundled under `examples/lexile_v2_artifacts/` (MIT license courtesy of Elizabeth Fawcett’s project). Download required NLTK corpora once via `poetry run python -m nltk.downloader punkt punkt_tab wordnet averaged_perceptron_tagger omw-1.4`.

---

## Examples

- `examples/example_corpus/chapter1.txt` – quick-start text excerpt.
- `examples/example_corpus/pg2701-images-3.epub` – Gutenberg EPUB for ingestion tests.
- `examples/run_lexile_v2_adapter.py`, `examples/run_openai_rewrite.py` – runnable scripts.
- `examples/meta/oer_sources.example.json` – template manifest for OER downloads.

---

## Rewriting with OpenAI

Install the `llm-openai` extra, enable `rewrite_enabled: true`, and use `scripts/load-openai-key.ps1` if you manage credentials via LastPass. CLI overrides (`--openai-model`, `--openai-temperature`, etc.) let you tune behavior per run.

---

## Testing

```bash
poetry run pytest
poetry run pyright
poetry run black --check .
poetry run isort --check-only .
```

> Use `pytest` or `python -m pytest`, **not** `python -m test`.

---

## Code Statistics

`cloc` binaries live in `tools/`. Run via helper scripts:

```bash
./scripts/run-cloc.sh
pwsh ./scripts/run-cloc.ps1
```

---

## Next Steps / Roadmap

1. **Docs consolidation (in process).** Fold `.github/text-difficulty-pipeline-plan.md` into docs/ and keep this README in sync.
2. **Corpus expansion.** Wire additional CC-BY/CC0 domains (NOAA, NASA, CDC) with dedicated download helpers + weighting.
3. **Advanced weighting.** Support shard-level weighting or stratified sampling during normalization to keep Gutenberg under a target share.
4. **Calibration diagnostics.** Add residual plots, per-band metrics, and automated reporting for regression fits.
5. **Packaging.** Publish the CLI entry points to PyPI once docs/tests stabilize.
6. **Evaluator benchmarks.** Add regression tests comparing analyzer estimates to known Lexile values.

Contributions and issue reports are welcome—open a PR or discussion if you build new corpus sources, calibration catalogs, or estimator improvements.
